---
title: "Was sind Nutzendenanforderungen an KI als Werkzeug zur Detektion von Misinformation auf Social Media? Wie gehen Personen in diesem Kontext mit unterschiedlichen Feedback-Arten um?"
author: 
  - Kaja Bartsch 
  - Zelda Becker
  - Lena Dung  
  - Anna Feldmann
  - Vanessa Obermüller
date: today 
format: 
  html: 
    toc: true
    code_folding: "hide"
  pdf: default
editor: visual
fontsize: 12pt
bibliography: "https://api.citedrive.com/bib/53a4a654-08eb-4d30-aa79-cc45a44d541d/references.bib?x=eyJpZCI6ICI1M2E0YTY1NC0wOGViLTRkMzAtYWE3OS1jYzQ1YTQ0ZDU0MWQiLCAidXNlciI6ICIxMDkzNSIsICJzaWduYXR1cmUiOiAiMzVhM2MyMjU0ODQ4NDBkNDI5NDI0NGFkYzBkMGQzOWNhOGEzYTU4YjQ5YTMzYjM4NGU2YmQ1OWUwOTMzMThlZCJ9"
csl: "apa (1).csl"
---

**GitHub Repository:** https:\\github.com:vanessaobermueller/SMNF2024-Aufsatz-GruppeA2.git **Fuer die Abgabe aktueller GitHub Hash:** f067746

## Code of Conduct

Wir verpflichten uns als Gruppe sorgfältig mit Feedback, unterschiedlichen Perspektiven und Meinungsverschiedenheiten umzugehen, in dem wir in Diskurs treten, einander zuhören und versuchen, gegensätzliche Meinungen zu verstehen. Des Weiteren teilen wir die Aufgaben gleichmäßig unter allen Autoren auf und verpflichten uns, an vereinbarten Terminen teilzunehmen. Durch eine Übersicht der eben genannten zu beachtenden Faktoren, auf die wir alle Zugriff haben, wird es uns möglich sein, auf einen Blick Termine und Aufgabenverteilungen zu sehen. Wir achten auf die Ehrlichkeit, Genauigkeit und Objektivität unserer Arbeit, indem wir verfasste Texte und Ergebnisse untereinander prüfen und uns gegebenenfalls auch Meinung von Dritten einholen, ohne vertrauliche Daten weiterzugeben und erstellen keine Plagiate. Letzteres überprüfen wir regelmäßig mit zuverlässigen Scannern für Plagiate. Wir manipulieren keine Daten. Dort beharren wir wieder auf gegenseitige Kontrolle. Wir werden die Daten schützen und wahren die Vertraulichkeit, indem Daten sicher gespeichert werden und vertrauliche Daten nicht an Dritte weitergegeben werden. Bei einer Nutzung von AI Tools werden wir diese kennzeichnen.

# 1 Einleitung

# 2 Literaturübersicht

In dem wissenschaftlichen Artikel “Fake news detection within online social media using supervised artificial intelligence algorithms” [@ozbay2020], berichten die Autoren davon, dass soziale Medien viele Vorteile, so wie z.B. geringe Kosten, einen einfachen Zugang zu Informationen und die schnelle Ausbreitung dieser, bieten. Daher sind sie zu einer beliebten Variante geworden, um Informationen zu erhalten. Viele Menschen tendieren dazu, eher Soziale Medien als Ressource für Nachrichten zu benutzen, anstatt klassische Nachrichtenquellen. Diese Quellen sind jedoch nicht qualifiziert. Im folgenden haben wir drei weitere Quellen betrachtet, welche sich mit dem Thema der Nutzung von Künstlicher Intelligenz als Detektion von Misinformationen beschäftigen.

Mit dem Anstieg von sozialen Medien als Informationsquelle, häuft sich das Vorkommen von Misinformationen, wobei es vielen User:innen schwer fällt diese zu erkennen, sagt der Artikel “Finding Strategies Against Misinformation in Social Media: A Qualitative Study” [@urakami2022]. Die Hauptprobleme der User:innen sind die Informationsflut, die Unterscheidung von richtigen und falschen Informationen und der negative Einfluss von Misinformationen auf die Emotionen. Es gibt auch einige Muster, die das Verhalten von User:innen repräsentieren, welche auf Misinformation treffen. Entweder werden diese ignoriert, es werden Freunde und Familie gewarnt, die Informationen überprüft und/oder die wichtigsten Informationen hervorgehoben. Generell kann man aber sagen, dass die meisten keine Strategie haben, wie sie mit Misinformationen umgehen oder diese überhaupt entdecken sollen. Eine Hilfe dabei können KI-Tools sein. Diese können die falschen Informationen entweder entfernen oder weniger sichtbar machen. Das Entfernen nimmt dem/der User:in die Kontrolle über das, was er/sie potenziell lesen möchte/kann. Eine andere Möglichkeit wäre, Warnungen an die Informationen zu knüpfen, wobei hier das Problem entsteht, dass man noch mehr Aufmerksamkeit auf die Misinformationen lenkt. Abgesehen davon können KI-Tools aber dabei helfen, dass User:innen schneller die Wahrhaftigkeit und Glaubwürdigkeit von Informationen erkennen und die Fähigkeit entwickeln, Misinformationen herauszufiltern.

Die Vorgehensweisen von Künstlicher Intelligenz sind aber laut dem Review “Fake news, disinformation and misinformation in social media: a review” [@aimeur2023] noch nicht in der Lage, die Probleme von Misinformation auf Social Media zu lösen. Insbesondere, da Beiträge mit Misinformation so gestaltet sind, dass sie der Wahrheit möglichst ähnlich sehen. Aus diesem Grund ist es für KI schwierig, diese ohne eine dritte Partei zu erkennen. Die Probleme können dabei in drei Kategorien geordnet werden: Content-based issues, contextual issues und issues of existing datasets. Hybrid-Techniken können dabei mehrere Probleme gleichzeitig betrachten, werden dadurch allerdings deutlich komplexer, sodass es schwierig werden kann, zu entscheiden, welche Informationen aus welchem Bereich relevant sind. Zu analysieren, wie Quellen und Verbreiter von Fake News über verschiedene Plattformen arbeiten, ist dabei essentiell, um Misinformation zu erkennen.

[@jahanbakhsh2023] untersucht die Kombination von subjektiven Einschätzungen von User:innen und Künstlicher Intelligenz zur Identifikation von Misinformation auf Social Media Plattformen. Die Autoren argumentieren, dass herkömmliche Ansätze zur Bekämpfung von Misinformationen aufgrund der Dynamik der sozialen Medienlandschaft unzureichend sind. Bei dem Ansatz der personalisierten KI interagiert diese mit den Teilnehmer:innen, lernt und analysiert Twitter-Feeds und versucht, eine Beurteilung vorherzusagen, welche an User:innen individuell angepasst ist. Die Diskussion betrachtet sowohl ihr Potential, äußert aber auch Bedenken. Dabei werden zu beachtende ethische Aspekte der Personalisierung und Umsetzungsmöglichkeiten beleuchtet. Des Weiteren werden Personalisierung und Zentralisierung solcher KI-Tools gegenübergestellt und Fragen über das Ausmaß der Transparenz der Entscheidung und Autonomie der User:innen aufgeworfen, sowohl als auch die Fähigkeit der User:innen, Misinformation im Rahmen des Lernprozesses zu erkennen und die Gefahren von Filterblasen. Die Ergebnisse zeigen Fortschritte in dem Gebiet der Bekämpfung von Misinformation, die Autoren betonen jedoch die große Notwendigkeit weiterer Forschung auf diesem Gebiet.

Zusammenfassend sieht man, dass es viele verschiedene Perspektiven zu dem Thema der Nutzung von KI zur Entdeckung von Misinformationen gibt. Zum einen könnten KI-Tools helfen, Misinformationen zu erkennen und gegen diese vorzugehen, jedoch sind diese aktuell oft noch nicht in der Lage diese sicher zu erkennen. Für unser Thema, also die Nutzungsanforderungen an entsprechende KI sind diese Artikel relevant, da sie mögliche Problemstellungen aufzeigen. Im ersten Artikel werden die aktuellen Probleme von Nutzern und der Erkennung von Misinformationen thematisiert, was wichtig ist, um diese Probleme beim Einsatz von KI vermeiden zu können. Damit die KI glaubwürdig auf die Nutzer wirkt, muss sie die Informationen richtig einstufen. Mit den Problemen die dabei entstehen, beschäftigt sich der zweite Artikel. Auch die Personalisierung ist ein wichtiges Thema, damit Nutzer gerne mit einem System arbeiten, welches der dritte Artikel aufgreift und problematisiert, wie mit Transparenz und Autonomie ethisch korrekt umgegangen werden kann.

# 3 Methodik

# 4 Ergebnisse



```{r code, echo = FALSE}
ati <- read.csv("mturk_data.csv", header = TRUE ,)
mean_age <- mean(ati$age)
mean_ati_young <- mean(ati[ati$age < mean_age,"ati_mean"])

mean_ati_old <- mean(ati[ati$age >= mean_age, "ati_mean"]) 

```

Jüngere Teilnehmer haben im Schnitt einen leicht höheren ATI Score von `r round(mean_ati_young,2)` als ältere Teilnehmer, welche einen ATI Score von `r round(mean_ati_old,2)` haben. Daraus folgt, dass beide Gruppen ein ähnlich hohes Ausmaß an Technikaffinität aufzeigen.


## 4.1 Qualitative Ergebnisse
Aus der thematischen Analyse der Transkripte ergeben sich folgende Themen, die relevant für unsere Forschungsfrage sind: Misinformationen, Eigenschaften, Grenzen, Verantwortung und Transparenz.

| Name  | Definition | Beleg |
|------|------|------|
| Misinformationen auf Social Media | Notwendigkeit eines KI-Tools, das Informationen im Internet beurteilt und dessen Nützlichkeit    | „Ehm, gerade auf Seiten wie Instagram und Tiktok, wo jeder posten kann was er will, muss man vorsichtig sein und darf nicht alles glauben, was einem präsentiert wird, da viele mit oder ohne Absicht andere beeinflussen, indem sie ihre Meinung verbreiten, weil diese von vielen sofort als Wahrheit angenommen wird.“ (A2_5, Zeile 45-49) „Ein ausreichend trainiertes KI System kann natürlich dabei helfen, schnell und ohne menschliches Eingreifen Misinformationen zu spotten und als solche zu flaggen, was es natürlich auch dem endgültigen User einfacher macht, Desinformation von tatsächlichen Fakten zu unterscheiden.“ (A2_4, Zeile 67-71)  |
| Eigenschaften des KI-Tools    | Anforderungen an das KI-Tool, damit es zufriedenstellend und zielführend benutzbar ist    | „Es muss natürlich korrekt sein, also es muss ja schon auf jeden Fall richtig funktionieren.“ (A2_3, Zeile 45-46) „Okay, also ich sprach ja von einem Label und ich stelle mir da so einen Popup-Button vor, dass man da unten in der Mitte vom Screen zum Beispiel sieht man einen Popup-Button, da steht ‚Hierbei handelt es sich wahrscheinlich um Misinformation‘, dann klicken wir da drauf und wunderbar wäre es natürlich, wenn thematisch das aufgegriffen wird, wobei es sich um Misinformation handle und das versucht wird mit Fakten zu unterstützen.“ (A2_1, Zeile 104-110) |
| Grenzen und Sorgen    | Grad, bis zu dem so ein System wirklich helfen und funktionieren kann    | Es kam die Frage auf, ob die Kennzeichnung von Fake News tatsächlich dabei helfen würde, Bewusstsein zu schaffen oder ob die Fronten doch nur verhärtet würden. (vgl. A2_3, Zeile 142-145) „Und deshalb denke ich, dass noch viel daran gearbeitet werden muss, dass so was halt wirklich ernst genommen werden kann, weil KI halt noch nicht alles versteht und deshalb auch oft falsche Antworten liefert.“ (A2_2, Zeile 95-98) | 
|Verantwortung für das KI-Tool | Partie, die sich um Durchführung kümmert | „Irgendein Medium, dass keinen Vorteil hat, wenn Misinfromationen verbreitet werden, also neutral dem gegenübersteht.“ (A2_5, Zeile 97-98) „Ich denke die Verantwortung hat a uf jeden Fall die Social Media Seite.“ (A2_4, Zeile 150-151) |
| Transparenz des KI-Tools | Beurteilungen des KI-Tools nachvollziehen können | „Bei einem transparenten KI-Tool ist es wichtig, welche Daten verwendet werden, um das neutrale Netz zu trainieren und welche Quellen die KI als seriöse Quellen wertet.“ (A2_5, Zeile 105-107) „Ich finde es sollte Optionen geben, vielleicht könnte es einfach die Informationen sozusagen liefern und dann auch noch Optionen geben, dass man sozusagen den Gedankengang, also auch wenn KIs keinen Gedankengang haben, aber den Gedankengang dahinter noch mal, um den nachvollziehen zu können.“ (A2_3, Zeile 104-108) |

Bei dem Thema „Misinformationen auf Social Media“ geht es darum, ob die Befragten ein KI-Tool dieser Art überhaupt als notwendig erachten. Alle Befragten haben angegeben, schon mal in Kontakt mit Misinformationen geraten zu sein. Außerdem wäre so ein System laut ihnen hilfreich und könne bis zu einem gewissen Grad definitiv dabei helfen, das Problem von Verbreitung von Misinformationen anzugehen.
Wie man das System gestalten sollte, dass es den Anforderungen der Nutzenden entspricht, wird mit dem Thema „Eigenschaften des KI-Tools“ beschrieben. Höchste Priorität hatte bei den Befragten die Zuverlässigkeit des Systems. Es muss neutrale, faktenbasierte Entscheidungen treffen können und mit seriösen Quellen arbeiten. Am wichtigsten sei es, dass das System keine richtigen Informationen als falsch markiert und dass es sowohl Bild, Video, Audio und Text prüft, um ein bestmögliches Ergebnis zu liefern. Wenn es um die direkte Nutzung geht, hatten mehrere Befragte die Vorstellung, dass die entsprechenden Posts mit einer Art Icon versehen werden, sodass man weiß, dass dieser Post Misinformationen enthält. Klickt man auf das Icon, wäre es optimal, wenn man noch mehr Informationen zu dem Thema bekommt oder die Option hat, sich selbst noch weiter zu informieren. Dieses Icon soll auch immer zu sehen sein und nicht erst auf Anfrage. Außerdem soll man die KI-Response melden können, im Falle einer falschen Markierung, aber mehr Interaktion muss mit dem System nicht möglich sein. Eine andere Idee für so ein System ist, dass man selbst Informationen in das System gibt und diese von der KI prüfen lässt und nicht, dass die KI automatisch immer alle Posts prüft. 
Es wurden auch einige kritische Gedanken geäußert, die im Thema „Grenzen und Sorgen“ festgehalten sind. Zum einen wurde hinterfragt, ob eine KI subjektive Meinungen, Satire und Sarkasmus erkennen und beurteilen kann. Außerdem wurde mehrfach geäußert, dass die KI auf jeden Fall „neutral“, „unparteiisch“ oder „unbiased“ entscheiden müsse und ob es generell überhaupt möglich sei, eine KI mit solchen Anforderungen zu entwickeln.
Zum Thema „Verantwortung für das KI-Tool“ wäre die optimale Lösung, dass es eine dritte, unabhängige Partei gäbe, die keinen Nutzen von Verbreitung oder Verbergung von Informationen hat. Da dies aber unrealistisch erscheint, läge die Verantwortung wohl bei den Social Media Seiten, wobei auch der Gedanke geäußert wurde, dass es Richtlinien vom Staat geben soll, an die sich die Social Media Seiten halten müssen. 
Als letztes geht es noch um das Thema „Transparenz des KI-Tools“. Man soll auf jeden Fall die Entscheidungen bzw. Beurteilungen der KI einsehen und nachvollziehen können und die Quellen, auf die sich die KI bezieht, überprüfen können. Außerdem seien Quelloffenheit des Codes und Transparenz im Datensatz, mit der die KI trainiert wird, wichtig.

## 4.2 Quantitative Ergebnisse 
@fig-codeati stellt den Zusammenhang zwischen ATI-1 und ATI-2 dar. 

```{r}
#| echo: false
#| message: false
#| fig-cap: "Zusammenhang zwischen ATI-1 und ATI-2"
#| label: fig-codeati

library(ggplot2)
library(tidyverse)

daten <- read.csv("mturk_data.csv")
ggplot(daten, aes(x = ati01, y = ati02)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE)+
  scale_x_continuous(limits = c(1,6), breaks = seq(1,6,1)) +
  scale_y_continuous(limits = c(1,6), breaks = seq(1,6,1))  
 
  
```


# 5 Diskussion

# 6 Literaturverzeichnis
<div id = "refs" ></div>

# 7 Anhänge
## Anhang 1 - Rekrutierungstext

Hallo zusammen!

Falschinformationen sind weit verbreitet und jeder ist mit Ihnen tagtäglich konfrontiert.\
In unserer Fragenbogenstudie testest du ein KI Werkzeugs (Künstliche Intelligenz), welches dir hilft irreführende Informationen im Internet zu erkennen.\
Die Studie dauert ca. 30 - 45 Minuten und kann einfach vom Laptop / Tablet / PC online durchgeführt werden.\
Medieninformatik- und Psychologie-Studierende der Universität Lübeck erhalten für die Teilnahme 0,5 VP Stunden.\ 

Unsere Einschlusskriterien sind

-	Mindestens 18 Jahre alt 

-	Gute Deutschkenntnisse

Der Link zur Teilnahme: insert Link lol\
Wir freuen uns auf deine Teilnahme!
